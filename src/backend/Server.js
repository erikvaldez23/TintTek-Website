require("dotenv").config({ path: require("path").resolve(__dirname, "../../.env") });
const express = require("express");
const cors = require("cors");
const fs = require("fs");
const mongoose = require("mongoose");
const { OpenAI } = require("openai");
const Message = require("./models/Message"); // Message model

const app = express();
app.use(cors());
app.use(express.json());

const PORT = process.env.PORT || 5001;
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// âœ… MongoDB Connection
mongoose.connect(process.env.MONGODB_URI)
  .then(() => console.log("âœ… Connected to MongoDB Atlas"))
  .catch((err) => console.error("âŒ MongoDB connection error:", err));

// âœ… File paths
const FAQ_PATH = __dirname + "/data/training_data.json";
const USER_MESSAGES_PATH = __dirname + "/data/user_messages.json";

// âœ… Store in JSON file (optional local backup)
const storeUserMessageToFile = (userMessage, botReply) => {
  const newEntry = {
    timestamp: new Date().toISOString(),
    userMessage,
    botReply,
  };

  let currentData = [];

  try {
    if (fs.existsSync(USER_MESSAGES_PATH)) {
      const raw = fs.readFileSync(USER_MESSAGES_PATH, "utf8");
      currentData = JSON.parse(raw);
    }
  } catch (err) {
    console.error("âŒ Error reading user messages file:", err);
  }

  currentData.push(newEntry);

  try {
    fs.writeFileSync(USER_MESSAGES_PATH, JSON.stringify(currentData, null, 2));
    console.log("ğŸ“ Stored user message to file.");
  } catch (err) {
    console.error("âŒ Error writing user message file:", err);
  }
};

// âœ… Load FAQ JSON
let faqData = [];
let faqEmbeddings = [];

const loadFAQData = () => {
  try {
    const data = fs.readFileSync(FAQ_PATH, "utf8");
    faqData = JSON.parse(data);
    console.log("âœ… Loaded FAQ Data:", faqData.length, "entries");
  } catch (error) {
    console.error("âŒ Error loading FAQ data:", error);
  }
};

// âœ… Get Embedding
const getEmbedding = async (text) => {
  try {
    const response = await openai.embeddings.create({
      model: "text-embedding-ada-002",
      input: text,
    });
    return response.data[0].embedding;
  } catch (error) {
    console.error("âŒ Error generating embedding:", error);
    return null;
  }
};

// âœ… Preload all FAQ Embeddings
const preloadEmbeddings = async () => {
  for (const faq of faqData) {
    const embedding = await getEmbedding(faq.question);
    if (embedding) {
      faqEmbeddings.push({ ...faq, embedding });
    }
  }
  console.log("âœ… Preloaded FAQ Embeddings");
};

// âœ… Cosine Similarity
const cosineSimilarity = (vecA, vecB) => {
  const dotProduct = vecA.reduce((sum, a, i) => sum + a * vecB[i], 0);
  const magnitudeA = Math.sqrt(vecA.reduce((sum, a) => sum + a * a, 0));
  const magnitudeB = Math.sqrt(vecB.reduce((sum, b) => sum + b * b, 0));
  return dotProduct / (magnitudeA * magnitudeB);
};

// âœ… Find Best Match
const findBestMatchAI = async (userMessage) => {
  const userEmbedding = await getEmbedding(userMessage);
  if (!userEmbedding) return null;

  let bestMatch = null;
  let highestSimilarity = -1;

  for (const faq of faqEmbeddings) {
    const similarity = cosineSimilarity(userEmbedding, faq.embedding);
    if (similarity > highestSimilarity) {
      highestSimilarity = similarity;
      bestMatch = faq;
    }
  }

  return { bestMatch, similarityScore: highestSimilarity };
};

// âœ… Generate Response
const generateAIResponse = async (userMessage) => {
  try {
    const { bestMatch, similarityScore } = await findBestMatchAI(userMessage);

    if (!bestMatch || similarityScore < 0.75) {
      return "I'm not totally sure about that â€“ for the most accurate answer, feel free to give us a call at (972) 362-8468 or check the Services page!";
    }

    const context = `Q: ${bestMatch.question}\nA: ${bestMatch.answer}`;

    const prompt = `
You are a helpful FAQ assistant. Use the FAQ data below to answer the user's question.
Keep it under 3 sentences. Be clear, professional, and concise.

### FAQ Data:
${context}

### User Question:
${userMessage}

### Answer:
`;

    const completion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "system",
          content:
            "You are a helpful customer support assistant. If no relevant FAQ is available, direct users to contact support. Answer clearly based on the FAQ context provided.",
        },
        { role: "user", content: prompt },
      ],
      max_tokens: 300,
    });

    return completion.choices[0].message.content.trim();
  } catch (error) {
    console.error("âŒ Error generating AI response:", error);
    return "Oops! Something went wrong. Please try again later or contact us directly.";
  }
};

// âœ… Load and Start Server
loadFAQData();

preloadEmbeddings().then(() => {
  app.listen(PORT, () => console.log(`ğŸš€ Server running on port ${PORT}`));
});

// âœ… Chat Endpoint
app.post("/chat", async (req, res) => {
  try {
    const { message } = req.body;
    console.log("ğŸ“© User Input:", message);

    const botReply = await generateAIResponse(message);

    // âœ… Store to MongoDB
    await Message.create({
      userMessage: message,
      botReply,
    });

    // âœ… Store to file (optional)
    storeUserMessageToFile(message, botReply);

    res.json({ reply: botReply });
  } catch (error) {
    console.error("âŒ Error in /chat endpoint:", error);
    res.status(500).json({
      reply: "Oops! Something went wrong. Please contact us at (972) 362-8468.",
    });
  }
});
